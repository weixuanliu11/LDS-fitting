{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural decoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pynwb\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from pynwb import NWBHDF5IO, NWBFile, TimeSeries\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "import bisect\n",
    "import ssm\n",
    "\n",
    "import autograd.numpy as np\n",
    "import autograd.numpy.random as npr\n",
    "\n",
    "\n",
    "from data_loaders import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Target Position (Cartesian Coordinate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classifier\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn import svm\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "def clf(method, feature, cluster_assignments):\n",
    "    '''\n",
    "    Do cross validation using the given data and classifer\n",
    "    :param method: the abbreviation of a classifier\n",
    "        should be one of knn, svm, rf, gbm, lr and nb\n",
    "    :param feature: feature(input of the classifier)\n",
    "    :param cluster_assignments: target id(expected output of the classifier)\n",
    "    :return: scores\n",
    "    '''\n",
    "    if method == 'knn':\n",
    "        clf = KNeighborsClassifier(n_neighbors=1)\n",
    "    elif method == 'svm':\n",
    "        clf = svm.SVC(kernel='linear')\n",
    "    elif method == 'rf':\n",
    "        clf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "    elif method == 'gbm':\n",
    "        clf = GradientBoostingClassifier(n_estimators=100, learning_rate=0.1, random_state=42)\n",
    "    elif method == 'lr':\n",
    "        clf = LogisticRegression(random_state=42)\n",
    "    elif method == 'nb':\n",
    "        clf = GaussianNB()\n",
    "    else:\n",
    "        print('There is no such method')\n",
    "\n",
    "    scores = cross_val_score(clf, feature, cluster_assignments, cv=15)\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CLFPerFile(filename, num_clusters, acqRate, newRate):\n",
    "    '''\n",
    "    Perform classification on data extracted from an NWB file.\n",
    "    :param filename: name of the NWB file\n",
    "    :param num_clusters: number of clusters for k-means clustering\n",
    "    :param acqRate: acquisition rate of data\n",
    "    :param newRate: new rate of data\n",
    "    :return: rObs: Mean classification accuracies for raw observations.\n",
    "             ls: Mean classification accuracies for latent states.\n",
    "             sObs: Mean classification accuracies for smoothed observations.\n",
    "             ls2d: Mean classification accuracies for 2D projections of latent states.\n",
    "\n",
    "    '''\n",
    "    #Extract data from the file\n",
    "    with NWBHDF5IO(filename , \"r\") as io:\n",
    "        read_nwbfile = io.read()    \n",
    "        trial_num = len(read_nwbfile.trials[\"id\"].data)\n",
    "        lastTrialId = getLastTrialId(read_nwbfile, acqRate=acqRate, newRate=newRate)\n",
    "        cursor_position_list=CursorPositionMatrix_list(read_nwbfile, \n",
    "                                                       lastTrialId, acqRate=acqRate, newRate = newRate)\n",
    "    last_cursor =[]\n",
    "    for cursor_position in cursor_position_list:\n",
    "        last_cursor.append(cursor_position[-1,:])\n",
    "    stop_Indices=[]\n",
    "    accumulated_idx=0\n",
    "    for cursor_position in cursor_position_list:\n",
    "        accumulated_idx += len(cursor_position)\n",
    "        stop_Indices.append(accumulated_idx-1)       \n",
    "    colors = ['blue', 'black','red','green','m','orange', 'gray','c']\n",
    "    cluster_assignments, center_xs, center_ys = Cluster(last_cursor, \n",
    "                                                        num_clusters=num_clusters, acqRate=acqRate, vis = False)\n",
    "    plt.figure()\n",
    "    for i, center in enumerate(center_xs):\n",
    "        plt.plot(center, center_ys[i], 'o', color=colors[i], label = f'center {i+1}')\n",
    "    plt.title('Target Position', fontsize = 20)\n",
    "    plt.legend()\n",
    "    \n",
    "    # Fit LDS\n",
    "    num_permutation = 5\n",
    "    original_input, original_spike, p_input_matrices, p_spike_matrices = getData_for_LDS(filename, \n",
    "                                                                                       num_permutation = num_permutation,\n",
    "                                                                                       acqRate=acqRate, newRate=newRate, num_clusters = num_clusters)\n",
    "    obs=original_spike\n",
    "    inputs = original_input\n",
    "    time_bins = obs.shape[0]\n",
    "    obs_dim = obs.shape[1]\n",
    "    input_dim = 4\n",
    "    state_dim = 4\n",
    "    \n",
    "    lds_inp = ssm.LDS(obs_dim, state_dim, M=input_dim, emissions=\"poisson\")\n",
    "    elbos, q = lds_inp.fit((obs).astype(int), inputs=inputs, num_iters=20)\n",
    "    state_means = q.mean_continuous_states[0]\n",
    "\n",
    "    # fixed point\n",
    "    A = lds_inp.dynamics.A\n",
    "    b = lds_inp.dynamics.b\n",
    "    B = lds_inp.dynamics.Vs\n",
    "    FPs=[]\n",
    "    for i in range(len(center_xs)):\n",
    "        center = np.array([center_xs[i], center_ys[i]])\n",
    "        inputs = np.concatenate([center, [0, 0]])\n",
    "        inputs = inputs.reshape(-1, 1)\n",
    "        x = np.linalg.solve(np.eye(A.shape[0])-A, (B @ inputs).reshape(state_dim,1) + b.reshape(state_dim,1))\n",
    "        FPs.append(x.reshape(1,-1))\n",
    "\n",
    "    FPs = np.array(FPs).reshape(len(center_xs), state_dim)\n",
    "    pca = PCA(n_components=2)\n",
    "    pca.fit(FPs)\n",
    "    FP_2d = pca.transform(FPs)\n",
    "    plt.figure()\n",
    "    for i in range(len(FP_2d)):\n",
    "        plt.scatter(FP_2d[i,0], FP_2d[i,1], s=200, marker = 'X', color = colors[i])\n",
    "    plt.title('Fixed Points', fontsize=15)\n",
    "    plt.xlabel('PC1', fontsize=15)\n",
    "    plt.ylabel('PC2', fontsize=15)\n",
    "    \n",
    "    ls_2d = []\n",
    "    for trialid, stop_idx in enumerate(stop_Indices):\n",
    "        cps = state_means[stop_idx:stop_idx+1,:]\n",
    "        points_2d = pca.transform(cps)\n",
    "        plt.plot(points_2d[:,0], points_2d[:,1], 'o', color = colors[cluster_assignments[trialid]])\n",
    "        ls_2d.append(points_2d[0])\n",
    "    ls_2d = np.array(ls_2d)\n",
    "    \n",
    "    # Get all 4 features\n",
    "    # Raw observations\n",
    "    last_original_spike = np.array([original_spike[idx] for idx in stop_Indices])\n",
    "    # Latent State\n",
    "    last_latent_state=np.array([state_means[idx] for idx in stop_Indices])\n",
    "    # Smoothed observations(inferred firing rate)\n",
    "    smoothed_obs = lds_inp.smooth(state_means, (obs).astype(int))\n",
    "    last_smooth_obs=np.array([smoothed_obs[idx] for idx in stop_Indices])\n",
    "    # 2d proj of last latent states\n",
    "    ls_2d\n",
    "\n",
    "    # For each feature, use all the 6 methods\n",
    "    methods = ['knn','svm', 'rf', 'gbm','lr','nb']\n",
    "    rObs = np.zeros(len(methods))\n",
    "    ls = np.zeros(len(methods))\n",
    "    sObs = np.zeros(len(methods))\n",
    "    ls2d = np.zeros(len(methods))\n",
    "    for idx, method in enumerate(methods):\n",
    "        rObs[idx] = np.mean(clf(method, last_original_spike, cluster_assignments))\n",
    "        ls[idx] = np.mean(clf(method, last_latent_state, cluster_assignments))\n",
    "        sObs[idx] = np.mean(clf(method, last_smooth_obs, cluster_assignments))\n",
    "        ls2d[idx] = np.mean(clf(method, ls_2d, cluster_assignments))\n",
    "\n",
    "    return rObs, ls, sObs, ls2d\n",
    "\n",
    "\n",
    "def plotClfs(matrix, dataname):\n",
    "    '''\n",
    "    Plot cross validation scores across clssifiers\n",
    "    :param matrix: array containing scores of each classifier\n",
    "    :param dataname: the type of input data you use to perform classification\n",
    "    '''\n",
    "    fontsize=15\n",
    "    means = np.mean(matrix, axis=0)\n",
    "    std = np.std(matrix, axis=0)\n",
    "    methods = ['knn','svm', 'rf', 'gbm','lr','nb']\n",
    "    width = 0.6\n",
    "    fig, ax = plt.subplots()\n",
    "    for idx in range(matrix.shape[1]):\n",
    "        ax.bar(methods[idx].upper(), means[idx], yerr=std[idx], width=width, edgecolor = 'black', capsize=15, color = 'gray')\n",
    "        # ax.scatter([methods[idx].upper()]*len(matrix), matrix[:,idx], color='none', edgecolor='gray', marker='o', s=50, label='_nolegend_')\n",
    "    plt.axhline(y=0.125, color='r', linestyle='--')\n",
    "    ax.set_ylabel('Loss', fontsize=fontsize)\n",
    "    ax.set_title('Cross Validation Scores Across Classifiers', fontsize=fontsize + 5)\n",
    "    ax.tick_params(axis='x', labelsize=fontsize)\n",
    "    ax.tick_params(axis='y', labelsize=fontsize)\n",
    "    fig.savefig(f'CV Scores Across Classifiers using {dataname}.svg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filenames = [\"sub-monk-g_ses-session0.nwb\",\n",
    "             \"sub-monk-g_ses-session1.nwb\",\n",
    "             \"sub-monk-g_ses-session2.nwb\",\n",
    "             \"sub-monk-g_ses-session3.nwb\",\n",
    "             \"sub-monk-g_ses-session4.nwb\",\n",
    "             \"sub-monk-j_ses-session0.nwb\",\n",
    "             \"sub-monk-j_ses-session1.nwb\",\n",
    "             \"sub-monk-j_ses-session2.nwb\"]\n",
    "robsMatrix=[]\n",
    "lsMatrix = []\n",
    "sobsMatrix = []\n",
    "ls2dMatrix=[]\n",
    "for filename in filenames:\n",
    "    if 'g' in filename:\n",
    "        acqRate = 60\n",
    "        newRate = 20\n",
    "        print('g file')\n",
    "    else:\n",
    "        acqRate = 200\n",
    "        newRate = 40\n",
    "        print('j file')\n",
    "    if filename == \"sub-monk-j_ses-session1.nwb\":\n",
    "        num_cluster = 6\n",
    "        print('The one with 6 targets')\n",
    "    else:\n",
    "        num_cluster = 8\n",
    "    rObs, ls, sObs, ls2d = CLFPerFile(filename, num_cluster, acqRate, newRate)\n",
    "    robsMatrix.append(rObs)\n",
    "    lsMatrix.append(ls)\n",
    "    sobsMatrix.append(sObs)\n",
    "    ls2dMatrix.append(ls2d)\n",
    "    \n",
    "robsMatrix = np.array(robsMatrix)\n",
    "lsMatrix = np.array(lsMatrix)\n",
    "sobsMatrix = np.array(sobsMatrix)\n",
    "ls2dMatrix = np.array(ls2dMatrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotClfs(robsMatrix, 'robs')\n",
    "plotClfs(lsMatrix, 'ls')\n",
    "plotClfs(sobsMatrix, 'sobs')\n",
    "plotClfs(ls2dMatrix, 'ls2d')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plotFeatures(method, robsMatrix, lsMatrix, sobsMatrix, ls2dMatrix):\n",
    "    '''\n",
    "    Plot cross validation scores across features\n",
    "    :param method: classifer\n",
    "    :param robsMatrix: mean classification accuracies for raw observations.\n",
    "    :param lsMatrix: mean classification accuracies for latent states.\n",
    "    :param sobsMatrix: mean classification accuracies for smoothed observations.\n",
    "    :param ls2dMatrix: mean classification accuracies for 2D projections of latent states.\n",
    "    '''\n",
    "    method = method.lower()\n",
    "    methods = ['knn','svm', 'rf', 'gbm','lr','nb']\n",
    "    if method in methods:\n",
    "        idx = methods.index(method)\n",
    "        matrix = np.vstack((robsMatrix[:,idx], lsMatrix[:,idx], sobsMatrix[:,idx], ls2dMatrix[:,idx]))\n",
    "        matrix = matrix.T\n",
    "        fontsize=15\n",
    "        means = np.mean(matrix, axis=0)\n",
    "        std = np.std(matrix, axis=0)\n",
    "        features = ['ROBS', 'LS', 'SOBS', 'LS2D']\n",
    "        width = 0.6\n",
    "        fig, ax = plt.subplots()\n",
    "        for idx in range(matrix.shape[1]):\n",
    "            ax.bar(features[idx].upper(), means[idx], yerr=std[idx], width=width, edgecolor = 'black', capsize=15, color = 'gray')\n",
    "            # ax.scatter([features[idx].upper()]*len(matrix), matrix[:,idx], color='none', edgecolor='gray', marker='o', s=50, label='_nolegend_')\n",
    "        # Draw a horizontal line at y = 0.125\n",
    "        plt.axhline(y=0.125, color='r', linestyle='--')\n",
    "        # plt.xticks(rotation=45)\n",
    "        ax.set_ylabel('Loss', fontsize=fontsize)\n",
    "        ax.set_title(f'Cross Validation Scores Across Features({method.upper()})', fontsize=fontsize + 5)\n",
    "        ax.tick_params(axis='x', labelsize=fontsize)\n",
    "        ax.tick_params(axis='y', labelsize=fontsize)\n",
    "        plt.savefig(f'Cross Validation Scores Across Features({method.upper()}).svg')\n",
    "    else:\n",
    "        print('This method is not included')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "methods = ['knn','svm', 'rf', 'gbm','lr','nb']\n",
    "for method in methods:\n",
    "    plotFeatures(method, robsMatrix, lsMatrix, sobsMatrix, ls2dMatrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Velocity ($v_x, v_y$)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score, GridSearchCV\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.kernel_ridge import KernelRidge\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.datasets import make_regression\n",
    "# filename = \"sub-monk-g_ses-session0.nwb\"\n",
    "filename = \"sub-monk-g_ses-session2.nwb\"\n",
    "# filename = \"sub-monk-g_ses-session3.nwb\"\n",
    "# filename = \"sub-monk-j_ses-session1.nwb\"\n",
    "num_permutation = 0\n",
    "acqRate=60\n",
    "newRate=20\n",
    "num_clusters = 8\n",
    "def LinearR(data, targets):\n",
    "    '''\n",
    "    Perform cross validation using Linear Regression and return averaged r2 score\n",
    "    :param data: inputs\n",
    "    :param targets: labels\n",
    "    :return: r2_avg\n",
    "    '''\n",
    "    model = LinearRegression()\n",
    "    r2s = cross_val_score(model, data, targets, cv=8, scoring='r2')\n",
    "    print('Linear score', r2s)\n",
    "    r2_avg = np.mean(r2s)\n",
    "    return r2_avg\n",
    "\n",
    "def kernelR(data, targets):\n",
    "    '''\n",
    "    Perform cross validation using Kernal Regression with optimal hyperparameters\n",
    "    and return averaged r2 score\n",
    "    :param data: inputs\n",
    "    :param targets: labels\n",
    "    :return: r2_avg\n",
    "    '''\n",
    "    param_grid = {\n",
    "        'alpha': [0.1, 1, 10],\n",
    "        'kernel': ['linear', 'poly', 'rbf']\n",
    "    }\n",
    "    grid_search = GridSearchCV(KernelRidge(), param_grid, cv=5, scoring='r2')\n",
    "    grid_search.fit(data, targets)\n",
    "    best_params = grid_search.best_params_\n",
    "    \n",
    "    kr = KernelRidge(**best_params)\n",
    "    r2s = cross_val_score(kr, data, targets, cv=8, scoring='r2')\n",
    "    print(\"Cross-Validation R^2 Scores:\", r2s)\n",
    "\n",
    "    print(\"Best Hyperparameters:\", best_params)\n",
    "    r2_avg = np.mean(r2s)\n",
    "    return r2_avg\n",
    "\n",
    "def svr(data, targets):\n",
    "    '''\n",
    "    Perform cross validation using Support Vector Machine with optimal hyperparameters\n",
    "    and return averaged r2 score\n",
    "    :param data: inputs\n",
    "    :param targets: labels\n",
    "    :return: r2_avg\n",
    "    '''\n",
    "    # Define hyperparameter grid\n",
    "    param_grid = {\n",
    "        'kernel': ['poly', 'rbf'],\n",
    "        'C': [0.1, 1, 10],\n",
    "        'gamma': [0.1, 1, 10]\n",
    "    }\n",
    "    grid_search = GridSearchCV(SVR(), param_grid, cv=5, scoring='r2')\n",
    "    grid_search.fit(data, targets)\n",
    "    best_params = grid_search.best_params_\n",
    "\n",
    "    svr = SVR(**best_params)\n",
    "\n",
    "    r2s = cross_val_score(svr, data, targets, cv=8, scoring='r2')\n",
    "    print(\"Cross-Validation R^2 Scores:\", r2s)\n",
    "    print(\"Mean Cross-Validation R^2:\", np.mean(r2s))\n",
    "\n",
    "    print(\"Best Hyperparameters:\", best_params)\n",
    "    print(\"Best R^2 Score:\", grid_search.best_score_)\n",
    "    r2_avg = np.mean(r2s)\n",
    "    return r2_avg\n",
    "\n",
    "with NWBHDF5IO(filename , \"r\") as io:\n",
    "    read_nwbfile = io.read()    \n",
    "    trial_num = len(read_nwbfile.trials[\"id\"].data)\n",
    "    lastTrialId = getLastTrialId(read_nwbfile, acqRate=acqRate, newRate=newRate)\n",
    "    cursor_position_list=CursorPositionMatrix_list(read_nwbfile, \n",
    "                                                   lastTrialId, acqRate=acqRate, newRate = newRate)\n",
    "last_cursor =[]\n",
    "for cursor_position in cursor_position_list:\n",
    "    last_cursor.append(cursor_position[-1,:])\n",
    "stop_Indices=[]\n",
    "\n",
    "# Get velocity at the end of each trial\n",
    "vs = []\n",
    "for cursor_position in cursor_position_list:\n",
    "    last_cp = np.array([cursor_position[-1,:], cursor_position[-4,:]])\n",
    "    v = np.diff(last_cp, axis=0) * newRate / 3\n",
    "    vs.append(v)\n",
    "vs=np.array(vs).reshape(-1,2)\n",
    "\n",
    "accumulated_idx=0\n",
    "for cursor_position in cursor_position_list:\n",
    "    accumulated_idx += len(cursor_position)\n",
    "    stop_Indices.append(accumulated_idx-1)       \n",
    "colors = ['blue', 'black','red','green','m','orange', 'gray','c']\n",
    "cluster_assignments, center_xs, center_ys = Cluster(last_cursor, \n",
    "                                                    num_clusters=num_clusters, acqRate=acqRate, vis = False)\n",
    "plt.figure()\n",
    "for i, center in enumerate(center_xs):\n",
    "    plt.plot(center, center_ys[i], 'o', color=colors[i], label = f'center {i+1}')\n",
    "plt.title('Target Position', fontsize = 20)\n",
    "plt.legend()\n",
    "\n",
    "# Fit LDS\n",
    "num_permutation = 5\n",
    "original_input, original_spike, p_input_matrices, p_spike_matrices = getData_for_LDS(filename, \n",
    "                                                                                   num_permutation = num_permutation,\n",
    "                                                                                   acqRate=acqRate, newRate=newRate, \n",
    "                                                                                     num_clusters = num_clusters)\n",
    "obs=original_spike\n",
    "inputs = original_input\n",
    "time_bins = obs.shape[0]\n",
    "obs_dim = obs.shape[1]\n",
    "input_dim = 4\n",
    "state_dim = 4\n",
    "\n",
    "lds_inp = ssm.LDS(obs_dim, state_dim, M=input_dim, emissions=\"poisson\")\n",
    "elbos, q = lds_inp.fit((obs).astype(int), inputs=inputs, num_iters=20)\n",
    "state_means = q.mean_continuous_states[0]\n",
    "\n",
    "# fixed point\n",
    "A = lds_inp.dynamics.A\n",
    "b = lds_inp.dynamics.b\n",
    "B = lds_inp.dynamics.Vs\n",
    "FPs=[]\n",
    "\n",
    "for i in range(len(center_xs)):\n",
    "    center = np.array([center_xs[i], center_ys[i]])\n",
    "    inputs = np.concatenate([center, [0, 0]])\n",
    "    inputs = inputs.reshape(-1, 1)\n",
    "    x = np.linalg.solve(np.eye(A.shape[0])-A, (B @ inputs).reshape(state_dim,1) + b.reshape(state_dim,1))\n",
    "    FPs.append(x.reshape(1,-1))\n",
    "\n",
    "FPs = np.array(FPs).reshape(len(center_xs), state_dim)\n",
    "pca = PCA(n_components=2)\n",
    "pca.fit(FPs)\n",
    "FP_2d = pca.transform(FPs)\n",
    "plt.figure()\n",
    "for i in range(len(FP_2d)):\n",
    "    plt.scatter(FP_2d[i,0], FP_2d[i,1], s=200, marker = 'X', color = colors[i])\n",
    "plt.title('Fixed Points', fontsize=15)\n",
    "plt.xlabel('PC1', fontsize=15)\n",
    "plt.ylabel('PC2', fontsize=15)\n",
    "\n",
    "ls_2d = []\n",
    "for trialid, stop_idx in enumerate(stop_Indices):\n",
    "    cps = state_means[stop_idx:stop_idx+1,:]\n",
    "    points_2d = pca.transform(cps)\n",
    "    plt.plot(points_2d[:,0], points_2d[:,1], 'o', color = colors[cluster_assignments[trialid]])\n",
    "    ls_2d.append(points_2d[0])\n",
    "ls_2d = np.array(ls_2d)\n",
    "\n",
    "# Get all 4 features\n",
    "# Raw observations\n",
    "last_original_spike = np.array([original_spike[idx] for idx in stop_Indices])\n",
    "# Latent State\n",
    "last_latent_state=np.array([state_means[idx] for idx in stop_Indices])\n",
    "# Smoothed observations(inferred firing rate)\n",
    "smoothed_obs = lds_inp.smooth(state_means, (obs).astype(int))\n",
    "last_smooth_obs=np.array([smoothed_obs[idx] for idx in stop_Indices])\n",
    "# 2d proj of last latent states\n",
    "ls_2d\n",
    "\n",
    "x_robs_lr = LinearR(last_original_spike, vs[:,0])\n",
    "x_ls_lr = LinearR(last_latent_state, vs[:,0])\n",
    "x_sobs_lr = LinearR(last_smooth_obs, vs[:,0])\n",
    "x_ls2d_lr = LinearR(ls_2d, vs[:,0])\n",
    "\n",
    "y_robs_lr = LinearR(last_original_spike, vs[:,1])\n",
    "y_ls_lr = LinearR(last_latent_state, vs[:,1])\n",
    "y_sobs_lr = LinearR(last_smooth_obs, vs[:,1])\n",
    "y_ls2d_lr = LinearR(ls_2d, vs[:,1])\n",
    "\n",
    "robs_kr = kernelR(last_original_spike, vs)\n",
    "ls_kr = kernelR(last_latent_state, vs)\n",
    "sobs_kr = kernelR(last_smooth_obs, vs)\n",
    "ls2d_kr = kernelR(ls_2d, vs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = ['linearRx', 'linearRy', 'kernerlR']\n",
    "plt.figure()\n",
    "plt.bar(labels, [x_robs_lr,y_robs_lr, robs_kr])\n",
    "plt.ylabel('R2')\n",
    "print('R2 of robs_kl', robs_kr)\n",
    "\n",
    "plt.figure()\n",
    "plt.bar(labels, [x_ls_lr,y_ls_lr, ls_kr])\n",
    "plt.ylabel('R2')\n",
    "\n",
    "plt.figure()\n",
    "plt.bar(labels, [x_sobs_lr,y_sobs_lr, sobs_kr])\n",
    "plt.ylabel('R2')\n",
    "print('R2 of sobs_kr', sobs_kr)\n",
    "\n",
    "plt.figure()\n",
    "plt.bar(labels, [x_ls2d_lr,y_ls2d_lr, ls2d_kr])\n",
    "plt.ylabel('R2')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
